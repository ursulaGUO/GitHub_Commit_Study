{"cells": [{"cell_type": "code", "execution_count": 2, "id": "3cb9c507-0ea0-494f-a65d-fc6c02537b71", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/12/09 01:01:42 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n24/12/09 01:01:42 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n24/12/09 01:01:42 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n24/12/09 01:01:43 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n24/12/09 01:01:45 WARN org.apache.spark.util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n24/12/09 01:01:50 WARN org.apache.spark.util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n24/12/09 01:01:52 WARN org.apache.spark.util.Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"}, {"name": "stdout", "output_type": "stream", "text": "3.1.3\n"}], "source": "import time\nimport os\nimport shutil\nfrom itertools import islice\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nimport findspark\nfindspark.init()\n\nfrom google.cloud import storage\n\nspark = SparkSession.builder \\\n    .appName(\"YourAppName\") \\\n    .config(\"spark.executor.memory\", \"8g\") \\\n    .config(\"spark.executor.cores\", \"4\") \\\n    .config(\"spark.driver.memory\", \"16g\") \\\n    .config(\"spark.driver.maxResultSize\", \"16g\") \\\n    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n    .config(\"spark.dynamicAllocation.minExecutors\", \"4\") \\\n    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n    .getOrCreate()\n\n\n\nspark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\nspark.conf.set(\"spark.sql.repl.eagerEval.maxCharsPerCell\", 200)\n\nprint(spark.version)"}, {"cell_type": "markdown", "id": "af7bf2d1-a623-4878-8dd2-07371c34acd4", "metadata": {}, "source": "# Read from folder into spark df\n\nYou will see 5 sub-folders, each containing a collection of parquet files.  A single folder can be read into Spark Dataframe:\n\n* Commits (gs://msca-bdp-data-open/final_project_git/commits): This contains information about the commits made to repositories. Each commit has metadata such as the author, committer, commit date, SHA, parent commit(s), and commit message.\n\n* Contents (gs://msca-bdp-data-open/final_project_git/contents): Provides the content of the files in the repositories. This is useful if you're looking to analyze source code or documents within repositories.\n\n* Files (gs://msca-bdp-data-open/final_project_git/files): This contains metadata about the files in the repositories such as the file path, the mode, and the blob ID which links back to the content.\n\n* Languages (gs://msca-bdp-data-open/final_project_git/languages): Each repository often has code written in one or more languages. This table provides an aggregation of the number of bytes of code for each language in a repository.\n\n* Licenses (gs://msca-bdp-data-open/final_project_git/licenses): Contains information on the licenses used by repositories.\n "}, {"cell_type": "code", "execution_count": 8, "id": "82c1ac1b-19c7-4e68-b8e3-04bf21c429b8", "metadata": {}, "outputs": [], "source": "#!hadoop fs -ls \"gs://msca-bdp-data-open/final_project_git/commits\""}, {"cell_type": "code", "execution_count": 9, "id": "0122fd55-896e-4110-9966-fe333a937fdc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Read into spark df\ncommits_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/commits\")\ncontents_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/contents\")\nfiles_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/files\")\nlanguages_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/languages\")\nlicenses_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/licenses\")"}, {"cell_type": "code", "execution_count": null, "id": "592deaf4-d527-4e3f-b60a-d6870d6ce105", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 8:====================>                               (1373 + 40) / 3484]\r"}], "source": "# Checking preliminary data\ncommits_spDf.limit(5)"}, {"cell_type": "markdown", "id": "ed9ee407-2704-4403-acfb-e9597c29bf87", "metadata": {}, "source": "# EDA\nWhat is the timeline of the data?  Do you see significant peaks and valleys?\n\nDo you see any data collection gaps?\n\nDo you see any outliers?  Remove obvious outliers before plotting the timeline\n\nDo you see any spikes?  Are these spikes caused by real activities / events?"}, {"cell_type": "code", "execution_count": null, "id": "6cc7cd41-e8be-44a4-9259-987aec8303eb", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}