{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2b078-43d4-48b6-a0db-44c02df18814",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8ae49-c073-4664-bcc5-15ff833e2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"4\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxCharsPerCell\", 200)\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7bf2d1-a623-4878-8dd2-07371c34acd4",
   "metadata": {},
   "source": [
    "# Read from folder into spark df\n",
    "\n",
    "You will see 5 sub-folders, each containing a collection of parquet files.  A single folder can be read into Spark Dataframe:\n",
    "\n",
    "* Commits (gs://msca-bdp-data-open/final_project_git/commits): This contains information about the commits made to repositories. Each commit has metadata such as the author, committer, commit date, SHA, parent commit(s), and commit message.\n",
    "\n",
    "* Contents (gs://msca-bdp-data-open/final_project_git/contents): Provides the content of the files in the repositories. This is useful if you're looking to analyze source code or documents within repositories.\n",
    "\n",
    "* Files (gs://msca-bdp-data-open/final_project_git/files): This contains metadata about the files in the repositories such as the file path, the mode, and the blob ID which links back to the content.\n",
    "\n",
    "* Languages (gs://msca-bdp-data-open/final_project_git/languages): Each repository often has code written in one or more languages. This table provides an aggregation of the number of bytes of code for each language in a repository.\n",
    "\n",
    "* Licenses (gs://msca-bdp-data-open/final_project_git/licenses): Contains information on the licenses used by repositories.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1ac1b-19c7-4e68-b8e3-04bf21c429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hadoop fs -ls \"gs://msca-bdp-data-open/final_project_git/commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122fd55-896e-4110-9966-fe333a937fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into spark df\n",
    "%time\n",
    "commits_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/commits\")\n",
    "contents_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/contents\")\n",
    "files_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/files\")\n",
    "languages_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/languages\")\n",
    "licenses_spDf = spark.read.parquet(\"gs://msca-bdp-data-open/final_project_git/licenses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592deaf4-d527-4e3f-b60a-d6870d6ce105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking preliminary data schema\n",
    "%time\n",
    "tables = [commits_spDf,contents_spDf,files_spDf,languages_spDf,licenses_spDf]\n",
    "\n",
    "for i in tables:\n",
    "    print(f\"Table \\n {i}\")\n",
    "    i.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ee407-2704-4403-acfb-e9597c29bf87",
   "metadata": {},
   "source": [
    "# EDA\n",
    "What is the timeline of the data?  Do you see significant peaks and valleys?\n",
    "\n",
    "Do you see any data collection gaps?\n",
    "\n",
    "Do you see any outliers?  Remove obvious outliers before plotting the timeline\n",
    "\n",
    "Do you see any spikes?  Are these spikes caused by real activities / events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7cd41-e8be-44a4-9259-987aec8303eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# Extract commit timestamp\n",
    "commits_spDf = commits_spDf.withColumn(\"author_timestamp\", from_unixtime(col(\"author.date.seconds\")))\n",
    "commits_spDf = commits_spDf.withColumn(\"committer_timestamp\", from_unixtime(col(\"committer.date.seconds\")))\n",
    "commits_spDf = commits_spDf.withColumn(\"author_commit_date\", col(\"author_timestamp\").cast(\"date\"))\n",
    "commits_spDf = commits_spDf.withColumn(\"committer_commit_date\", col(\"committer_timestamp\").cast(\"date\"))\n",
    "\n",
    "# View sample data\n",
    "commits_spDf.select(\"committer_timestamp\", \"author_timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95d1ba7-4839-4479-aaef-8c726052acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_spDf.select(\"committer_commit_date\", \"author_commit_date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84ef5d4-39a5-463b-b78e-e1841a07dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Aggregate by date\n",
    "author_timeline_df = commits_spDf.groupBy(\"author_commit_date\").agg(count(\"*\").alias(\"author_commit_count\"))\n",
    "committer_timeline_df = commits_spDf.groupBy(\"committer_commit_date\").agg(count(\"*\").alias(\"committer_commit_count\"))\n",
    "\n",
    "# Sort by date for visualization\n",
    "author_timeline_df = author_timeline_df.orderBy(\"author_commit_date\")\n",
    "author_timeline_df.show(10)\n",
    "\n",
    "committer_timeline_df = committer_timeline_df.orderBy(\"committer_commit_date\")\n",
    "committer_timeline_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805da32c-f6f5-41ea-8321-f4e63c4bb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_timeline_pdDf = author_timeline_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75d219-7a5a-4133-a941-3a8cc3a59c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(author_timeline_pdDf.head(5))\n",
    "display(author_timeline_pdDf.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539db14f-7a56-4789-8cb1-6d57883d9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "committer_timeline_pdDf = committer_timeline_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece025a-e84c-45c6-aca0-5923346457c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(committer_timeline_pdDf.head(5))\n",
    "display(committer_timeline_pdDf.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05362e86-8c91-4ee6-8080-6a84d1613f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDf = [author_timeline_pdDf, committer_timeline_pdDf]\n",
    "titles = ['author','committer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788d2bb-1e1c-4d34-91cd-075160146fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, val in zip(pdDf, titles):\n",
    "    df['Date'] = pd.to_datetime(df[f'{val}_commit_date'])\n",
    "    #df = df.dropna(inplace=True)\n",
    "    print(f\"For table {val}\")\n",
    "    print(f\"Earliest commit date in record is {df['Date'].min()}\")\n",
    "    print(f\"Latest commit date in record is {df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72bfae-f846-4b32-bde6-0b60cbf1dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, title in zip(pdDf, titles):\n",
    "    x_col = f'{title}_commit_date'\n",
    "    y_col = f'{title}_commit_count'\n",
    "    \n",
    "    df[x_col] = pd.to_datetime(df[x_col])\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df[x_col], df[y_col], marker='o', label=f'{title} timeline')\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Commit Date')\n",
    "    plt.ylabel('Commit Count')\n",
    "    plt.title(f'{title.capitalize()} Commit Timeline')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd04f3-deb9-4493-9935-4565ba4d54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "committer_timeline_pdDf.tail(5560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec8694-76df-4b89-8d51-5247861f6ac6",
   "metadata": {},
   "source": [
    "# Remove outliar that is likely to be errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3da877-abe4-42db-80e5-22b62e1dddf6",
   "metadata": {},
   "source": [
    "Now notice that there are a lot of future dates after 2022-11-27 that have only 2 counts. We don't want to see because they are likely errors. So we decide to filter out dates on and after 2022-11-27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304f472-702e-4dba-91c7-91a2a19f8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the dates are before 2022-11-27\n",
    "from pyspark.sql.functions import year,col\n",
    "\n",
    "cutoff_date = \"2022-11-27\"\n",
    "commits_spDf = commits_spDf.filter((col(\"author_timestamp\") < cutoff_date) & (col(\"committer_timestamp\") < cutoff_date))\n",
    "\n",
    "# View the filtered data\n",
    "commits_spDf.select(\"committer_timestamp\", \"author_timestamp\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3277da-1f4a-4ab5-8d9e-51864b2f2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by date\n",
    "author_timeline_df = commits_spDf.groupBy(\"author_commit_date\").agg(count(\"*\").alias(\"author_commit_count\"))\n",
    "committer_timeline_df = commits_spDf.groupBy(\"committer_commit_date\").agg(count(\"*\").alias(\"committer_commit_count\"))\n",
    "\n",
    "# Sort by date for visualization\n",
    "author_timeline_df = author_timeline_df.orderBy(\"author_commit_date\")\n",
    "author_timeline_df.show(10)\n",
    "\n",
    "committer_timeline_df = committer_timeline_df.orderBy(\"committer_commit_date\")\n",
    "committer_timeline_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdb3d7-bfe1-42e9-a6ce-62b8c69ebb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_timeline_pdDf_filtered = author_timeline_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6b446-1489-4611-8723-6433ab094ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "committer_timeline_pdDf_filtered = committer_timeline_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df1db9-d9cb-4817-acf8-a8215795d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "committer_timeline_pdDf_filtered.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f8cc6-9771-4849-be41-6be61668df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdDf = [author_timeline_pdDf_filtered, committer_timeline_pdDf_filtered]\n",
    "titles = ['author','committer']\n",
    "\n",
    "for df, title in zip(pdDf, titles):\n",
    "    x_col = f'{title}_commit_date'\n",
    "    y_col = f'{title}_commit_count'\n",
    "    \n",
    "    df[x_col] = pd.to_datetime(df[x_col])\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df[x_col], df[y_col], marker='o',s=10, label=f'{title} timeline')\n",
    "    \n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Commit Date')\n",
    "    plt.ylabel('Commit Count')\n",
    "    plt.title(f'{title.capitalize()} Commit Timeline')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4746f3d-12a8-4397-8792-75931996abce",
   "metadata": {},
   "source": [
    "# Popular Language Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e196a0b-4d1b-4152-a591-c23eccb20476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Step 1: Explode repo_name in commits_spDf\n",
    "commits_exploded = commits_spDf.withColumn(\"repo_name_exploded\", explode(col(\"repo_name\")))\n",
    "\n",
    "# Step 2: Join languages_spDf and exploded commits_spDf on repo_name\n",
    "merged_df = languages_spDf.alias(\"lang\").join(\n",
    "    commits_exploded.alias(\"commit\"),\n",
    "    col(\"lang.repo_name\") == col(\"commit.repo_name_exploded\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Step 3: Select relevant columns including time columns\n",
    "result_df = merged_df.select(\n",
    "    col(\"lang.repo_name\").alias(\"repo_name\"),           # from languages_spDf\n",
    "    col(\"lang.language\").alias(\"language\"),            # from languages_spDf\n",
    "    col(\"lang.language.bytes\").alias(\"language_bytes\"), # language bytes\n",
    "    col(\"commit.author_commit_date\").alias(\"author_commit_date\"), # from commits_spDf\n",
    "    col(\"commit.committer_commit_date\").alias(\"committer_commit_date\"), # from commits_spDf\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dbbe8a-913a-4a40-b463-4e7e2ddb4d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_time = result_df.groupBy([\"committer_commit_date\",\"language\"]).agg(count(\"*\").alias(\"Count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdfa99-ebc8-4f0f-a90f-731e78866b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert to Pandas for aggregation and plotting\n",
    "result_pd = language_time.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31bdaf-59ee-4a18-a7eb-866fabd63197",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_write = \"gs://msca-bdp-students-bucket/shared_data/xiuan/final_language_count.csv\"\n",
    "result_pd = result_pd.to_csv(bucket_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb6488-d714-482e-9a73-30146d219f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7c4ad-aaf8-4e93-be5c-c54c628980fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 5: Parse dates and aggregate\n",
    "result_pd[\"author_commit_date\"] = pd.to_datetime(result_pd[\"author_commit_date\"])\n",
    "result_pd[\"year_month\"] = result_pd[\"author_commit_date\"].dt.to_period(\"M\")  # Group by year and month\n",
    "\n",
    "# Aggregate data by language and time\n",
    "trend_data = result_pd.groupby([\"year_month\", \"language\"]).agg({\n",
    "    \"language_bytes\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "# Step 6: Save aggregated data to Pandas for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare data for plotting\n",
    "top_languages = trend_data.groupby(\"language\")[\"language_bytes\"].sum().nlargest(5).index  # Top 5 languages\n",
    "trend_plot_data = trend_data[trend_data[\"language\"].isin(top_languages)]\n",
    "\n",
    "# Plot trends\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(\n",
    "    data=trend_plot_data,\n",
    "    x=\"year_month\",\n",
    "    y=\"language_bytes\",\n",
    "    hue=\"language\"\n",
    ")\n",
    "plt.title(\"Trends of Most Popular Programming Languages Over Time\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Total Bytes\")\n",
    "plt.legend(title=\"Language\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36456e-1749-4976-a4e9-858c8ff88aa5",
   "metadata": {},
   "source": [
    "# License distribution\n",
    "What is the distribution of licenses across GitHub repositories?\n",
    "Any certain programming languages that are more likely to be associated with a particular license?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48100416-9da7-4b63-93be-58b2df8cf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "license_count = license_spDf.withColumn(\"Count\",count('License'))\n",
    "license_count.to_Panda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
